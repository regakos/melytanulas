# melytanulas
Question answering with DistilBERT

Team name: MMRASG 

Team members' names and Neptun codes: 
Molnár Márk - B44W74;
Regényi Ákos - OWPAZM;
Sáfrán Gergely- FT6QWV; 

Functions of the files in the repository, 
related works (papers, GitHub repositories, blog posts, etc),
and how to run it (building and running the container, running your solution within the container).

Project description:
Our project is using the DistilBERT transformer model for question answering. There are different types of question answering, in our work we give text to the model as input and a related (in-context) question. The model gives the proper answer to de predefined question. We create a user interface to input the data. The model is trained on the ... dataset. 


DistilBERT link: https://arxiv.org/pdf/1910.01108.pdf

https://www.kaggle.com/code/arunmohan003/question-answering-using-bert

https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model#:~:text=BERT%2C%20which%20stands%20for%20Bidirectional,calculated%20based%20upon%20their%20connection.

https://medium.com/analytics-vidhya/question-answering-system-with-bert-ebe1130f8def

