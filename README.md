# melytanulas
Question answering with DistilBERT

Team name: MMRASG 

Team members' names and Neptun codes: 
Molnár Márk - B44W74;
Regényi Ákos - OWPAZM;
Sáfrán Gergely- FT6QWV; 

Functions of the files in the repository: 
Dockerfile for building the docker image
requirements.txt: for defining the needed packages

Project description:
Our project is using the DistilBERT transformer model for question answering. There are different types of question answering, in our work we give text to the model as input and a related (in-context) question. The model gives the proper answer to de predefined question. We create a user interface to input the data. The model is trained on the ... dataset. 


related works (papers, GitHub repositories, blog posts, etc),:
BERT link: https://arxiv.org/abs/1810.04805

DistilBERT link: https://arxiv.org/pdf/1910.01108.pdf

Bert explained: https://medium.com/analytics-vidhya/question-answering-system-with-bert-ebe1130f8def

https://www.kaggle.com/code/arunmohan003/question-answering-using-bert

https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model#:~:text=BERT%2C%20which%20stands%20for%20Bidirectional,calculated%20based%20upon%20their%20connection.

How to run it:


